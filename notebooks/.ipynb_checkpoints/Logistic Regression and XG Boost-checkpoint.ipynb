{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get data\n",
    "\n",
    "data_path = '../data/training_test_data/'\n",
    "data = pd.read_csv(data_path + 'after_stock_data.csv')\n",
    "colnames = ['headline','source','label']   \n",
    "data.columns = colnames\n",
    "\n",
    "data2 = pd.read_csv(data_path + 'labeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>source</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple  Amazon  YouTube  Safer  faster  amp  le...</td>\n",
       "      <td>USA TODAY</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G Stocks  Will Amazon Dip Its Toes In</td>\n",
       "      <td>InvestorPlace</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fear of Amazon Creates a Bargain in FedEx Stock</td>\n",
       "      <td>InvestorPlace</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Amazon Buyout Buzz Draws Options Bulls to Grub...</td>\n",
       "      <td>Schaeffer's Investment Research</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US Stock Market Overview   Stocks Surge With O...</td>\n",
       "      <td>FX Empire</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Amazon Shoppers Are Obsessed With This Air Fry...</td>\n",
       "      <td>Cooking Light</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>a style  font size   px   href  https   r sea...</td>\n",
       "      <td>Deadline</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Amazon Stock Will Probably Test        but It ...</td>\n",
       "      <td>InvestorPlace</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Scouted  Save     On Top Rated Dehumidifiers D...</td>\n",
       "      <td>The Daily Beast</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>a style  font size   px   href  https   r sea...</td>\n",
       "      <td>Fashionista</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            headline  \\\n",
       "0  Apple  Amazon  YouTube  Safer  faster  amp  le...   \n",
       "1             G Stocks  Will Amazon Dip Its Toes In    \n",
       "2    Fear of Amazon Creates a Bargain in FedEx Stock   \n",
       "3  Amazon Buyout Buzz Draws Options Bulls to Grub...   \n",
       "4  US Stock Market Overview   Stocks Surge With O...   \n",
       "5  Amazon Shoppers Are Obsessed With This Air Fry...   \n",
       "6   a style  font size   px   href  https   r sea...   \n",
       "7  Amazon Stock Will Probably Test        but It ...   \n",
       "8  Scouted  Save     On Top Rated Dehumidifiers D...   \n",
       "9   a style  font size   px   href  https   r sea...   \n",
       "\n",
       "                            source  label  \n",
       "0                        USA TODAY      0  \n",
       "1                    InvestorPlace      0  \n",
       "2                    InvestorPlace      0  \n",
       "3  Schaeffer's Investment Research      0  \n",
       "4                        FX Empire      0  \n",
       "5                    Cooking Light      0  \n",
       "6                         Deadline      0  \n",
       "7                    InvestorPlace      0  \n",
       "8                  The Daily Beast      0  \n",
       "9                      Fashionista      0  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data cleansing and concatenation\n",
    "\n",
    "for i, row in data[data['source'].str.find(' via ') > -1].iterrows():\n",
    "  data.at[i,'source'] = data.at[i,'source'][:data.at[i,'source'].find(' via ')].strip()\n",
    "\n",
    "sources = []\n",
    "headlines = []\n",
    "\n",
    "for i, row in data2.iterrows():\n",
    "    sources.append(data2.at[i, 'headlines'][len(data2.at[i, 'headlines'])-data2.at[i, 'headlines'][::-1].find('-'):].strip())\n",
    "    headlines.append(data2.at[i, 'headlines'][:len(data2.at[i, 'headlines'])-data2.at[i, 'headlines'][::-1].find('-')-1].strip())                                \n",
    "    \n",
    "sources[:10] \n",
    "headlines[:10] \n",
    "\n",
    "clean_data2 = pd.DataFrame({'headline':headlines, 'source':sources, 'label':data2['relevancy']}, columns=colnames)\n",
    "\n",
    "df_union = pd.concat([data, clean_data2])\n",
    "\n",
    "df_union['headline'] = df_union['headline'].str.replace(\"[^a-zA-Z]\", \" \").replace(\"\\t\", \" \")\n",
    "\n",
    "df_union.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "nlp = English()\n",
    "\n",
    "spacy_stopwords = STOP_WORDS\n",
    "punct = string.punctuation\n",
    "    \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    return text.strip().lower()\n",
    "\n",
    "def tokenizer_spacy(headline):\n",
    "        #filtered_tokenized.append(df_union.at[i, 'headline'])\n",
    "        #filtered_tokenized.append(row)\n",
    "\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(str(headline))\n",
    "    \n",
    "    #lemmatization improved model\n",
    "    \n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop wordsspacy_stopwords\n",
    "    return [ str(word).lower().strip() for word in mytokens if str(word).lower().strip() not in spacy_stopwords and str(word).lower().strip() not in punct ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = tokenizer_spacy, ngram_range=(1,1))\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = tokenizer_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_union['headline'] # the features we want to analyze\n",
    "y= df_union['label'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x0000026B9FBF7320>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.7577828397873956\n",
      "Logistic Regression Precision: 0.7493857493857494\n",
      "Logistic Regression Recall: 0.8413793103448276\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7)\n",
    "\n",
    "text = Pipeline([\n",
    "                ('selector', TextSelector(key='description')),\n",
    "                ('vectorizer', vec_tdidf)\n",
    "                ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
