{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>amazon opens preorders for cinderella annivers...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>google researchers add attention to augment co...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>android q the choice of the final name for goo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>google to phase out works with nestprogram in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>google announces major android auto update wit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              headline  label\n",
       "438  amazon opens preorders for cinderella annivers...      1\n",
       "439  google researchers add attention to augment co...      1\n",
       "440  android q the choice of the final name for goo...      1\n",
       "441  google to phase out works with nestprogram in ...      1\n",
       "442  google announces major android auto update wit...      1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = '../data/training_test_data/'\n",
    "data = pd.read_csv(data_path + 'after_stock_data_cleaned.csv')\n",
    "colnames = ['headline','source','label']   \n",
    "data.columns = colnames\n",
    "\n",
    "data2 = pd.read_csv(data_path + 'now_data_parsed.csv')\n",
    "\n",
    "data2.columns = ['headline','label']\n",
    "\n",
    "#getting data we have labeled out of the unlabeled data\n",
    "\n",
    "data2_labeled = pd.concat([data2[438:2438], data2[6448:8448],data2[12458:14458],data2[18468:20709],data2[24478:26479]], sort=False)\n",
    "\n",
    "data2_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25672</th>\n",
       "      <td>amazon upgrades fire 7 tablet with faster soc ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12501</th>\n",
       "      <td>deal the apple watch series 4 models are avail...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19237</th>\n",
       "      <td>amazon knocked off top of uk consumer poll onc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>Trump vows rapid, high tariffs on Mexico unles...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>The Downton Abbey Movie Full Trailer Is Here</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline  label\n",
       "25672  amazon upgrades fire 7 tablet with faster soc ...      1\n",
       "12501  deal the apple watch series 4 models are avail...      1\n",
       "19237  amazon knocked off top of uk consumer poll onc...      1\n",
       "1111   Trump vows rapid, high tariffs on Mexico unles...      0\n",
       "274         The Downton Abbey Movie Full Trailer Is Here      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#concatenate our data to their data\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = pd.DataFrame({'headline':data['headline'], 'label':data['label']}, columns=['headline', 'label'])\n",
    "\n",
    "df_union = shuffle(pd.concat([data, data2_labeled], sort=False))\n",
    "\n",
    "df_union.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing\n",
    "nlp = English()\n",
    "\n",
    "spacy_stopwords = STOP_WORDS\n",
    "punct = string.punctuation\n",
    "    \n",
    "class predictors(TransformerMixin):\n",
    "    def transform(self, X, **transform_params):\n",
    "        # Cleaning Text\n",
    "        return [clean_text(text) for text in X]\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {}\n",
    "\n",
    "# Basic function to clean the text\n",
    "def clean_text(text):\n",
    "    # Removing spaces and converting text into lowercase\n",
    "    try:\n",
    "        text = unidecode.unidecode(text)\n",
    "    except:\n",
    "        pass\n",
    "    return text.strip().lower()\n",
    "\n",
    "def tokenizer_spacy(headline):\n",
    "        #filtered_tokenized.append(df_union.at[i, 'headline'])\n",
    "        #filtered_tokenized.append(row)\n",
    "\n",
    "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
    "    mytokens = nlp(str(headline))\n",
    "    \n",
    "    #lemmatization improved model\n",
    "    \n",
    "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
    "\n",
    "    # Removing stop wordsspacy_stopwords\n",
    "    return [ str(word).lower().strip() for word in mytokens if str(word).lower().strip() not in spacy_stopwords and str(word).lower().strip() not in punct ]\n",
    "\n",
    "    # return preprocessed list of tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###SVD dimensionality reduction\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, algorithm='randomized', n_iter=5, random_state=None, tol=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vector = CountVectorizer(tokenizer = tokenizer_spacy, ngram_range=(1,1))\n",
    "tfidf_vector = TfidfVectorizer(tokenizer = tokenizer_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df_union['headline'] # the features we want to analyze\n",
    "y = df_union['label'] # the labels, or answers, we want to test against\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x000001AFFB28DA90>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ...penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "# Create pipeline using Bag of Words\n",
    "pipe = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('classifier', classifier)])\n",
    "\n",
    "# model generation\n",
    "pipe.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.8776876982728234\n",
      "Logistic Regression Precision: 0.9010812975570685\n",
      "Logistic Regression Recall: 0.9574468085106383\n",
      "Logistic Regression f1: 0.9284093253558903\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "# Predicting with a test dataset\n",
    "predicted = pipe.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"Logistic Regression Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"Logistic Regression Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"Logistic Regression Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"Logistic Regression f1:\",metrics.f1_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis = pd.DataFrame(X_test.copy())\n",
    "error_analysis['actual_classification'] = y_test\n",
    "error_analysis['logistic_regression_classification'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('cleaner', <__main__.predictors object at 0x000001AFFB856E10>), ('vectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "      ...alpha=0, reg_lambda=1, scale_pos_weight=1, seed=2, silent=None,\n",
       "       subsample=0.7, verbosity=1))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "clf = XGBClassifier(random_state=42, seed=2, colsample_bytree=0.6, subsample=0.7)\n",
    "\n",
    "text = Pipeline([(\"cleaner\", predictors()),\n",
    "                 ('vectorizer', bow_vector),\n",
    "                 ('selector', svd),\n",
    "                 ('classifier', clf)])\n",
    "\n",
    "text.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.8783926683115968\n",
      "XGBoost Precision: 0.8878143133462283\n",
      "XGBoost Recall: 0.9765957446808511\n",
      "XGBoost f1: 0.9300911854103343\n"
     ]
    }
   ],
   "source": [
    "predicted = text.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print(\"XGBoost Accuracy:\",metrics.accuracy_score(y_test, predicted))\n",
    "print(\"XGBoost Precision:\",metrics.precision_score(y_test, predicted))\n",
    "print(\"XGBoost Recall:\",metrics.recall_score(y_test, predicted))\n",
    "print(\"XGBoost f1:\",metrics.f1_score(y_test, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis['xgboost_classification'] = predicted\n",
    "error_analysis.to_csv(r\"C:\\Users\\amber\\Documents\\VSE\\DAEN 690\\repo\\DAEN-690\\data\\output_data\\error_analysis.csv\")\n",
    "#error_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 240,  247],\n",
       "       [ 100, 2250]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plot confusion matrices\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(error_analysis['actual_classification'], error_analysis['logistic_regression_classification'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 197,  290],\n",
       "       [  55, 2295]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(error_analysis['actual_classification'], error_analysis['xgboost_classification'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
